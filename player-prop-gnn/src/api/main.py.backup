"""
FastAPI Application for Player Prop Prediction
Phase 4.3 - WITH MONITORING

Changes from 4.1:
+ Structured JSON logging
+ Prometheus metrics tracking
+ Request ID middleware
+ /metrics endpoint
"""
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, Response
from pathlib import Path
import logging
from datetime import datetime
from typing import Optional
import time

from src.models.inference import BayesianPredictor, ModelNotLoadedError
from src.config.settings import settings
from .schemas import (
    PlayerPredictionRequest,
    PlayerPredictionResponse,
    PropPrediction,
    ErrorResponse
)
from .middleware import setup_middleware
from src.utils.logging import setup_logging, RequestLogger
from src.api.metrics import (
    get_metrics,
    initialize_metrics,
    record_cache_hit,
    record_cache_miss,
    record_cache_error,
    record_prediction_error,
    MODEL_LOADED,
    REDIS_CONNECTED,
    INFERENCE_DURATION,
    TimedContext
)
import redis
import json

# ============================================================================
# CONFIGURE LOGGING (Before anything else)
# ============================================================================

# Use JSON logging in production, plain text in development
format_json = settings.LOG_LEVEL.upper() != 'DEBUG'
logger = setup_logging(
    level=settings.LOG_LEVEL,
    format_json=format_json,
    log_file=None  # Docker captures stdout
)

# ============================================================================
# GLOBAL STATE
# ============================================================================

MODEL: Optional[BayesianPredictor] = None
REDIS_CLIENT: Optional[redis.Redis] = None

# ============================================================================
# CREATE FASTAPI APP
# ============================================================================

app = FastAPI(
    title="Player Prop Prediction API",
    description="Bayesian multi-task model for football player prop predictions",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Add monitoring middleware (request tracking, metrics, error handling)
setup_middleware(app)

# ============================================================================
# LIFECYCLE EVENTS
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """
    Load model and connect to Redis on application startup.
    
    Changes from 4.1:
    + Initialize Prometheus metrics
    + Update MODEL_LOADED and REDIS_CONNECTED gauges
    + Use structured logging
    """
    global MODEL, REDIS_CLIENT
    
    # Initialize metrics
    initialize_metrics()
    
    logger.info("=" * 60)
    logger.info("STARTING PLAYER PROP API WITH MONITORING")
    logger.info("=" * 60)
    
    # ========================================
    # LOAD MODEL (REQUIRED)
    # ========================================
    
    model_path = settings.model_path / "bayesian_multitask_v1.0.pkl"
    trace_path = settings.model_path / "bayesian_multitask_v1.0_trace.nc"
    
    logger.info(f"Loading model from {model_path}")
    
    try:
        start_time = time.time()
        MODEL = BayesianPredictor(
            str(model_path),
            str(trace_path),
            n_samples=1000
        )
        load_time_ms = int((time.time() - start_time) * 1000)
        
        # Update metric
        MODEL_LOADED.set(1)
        
        logger.info(
            "Model loaded successfully",
            extra={
                'load_time_ms': load_time_ms,
                'n_samples': MODEL.n_samples,
                'n_features': len(MODEL.feature_names)
            }
        )
        
    except FileNotFoundError as e:
        MODEL_LOADED.set(0)
        logger.critical(
            "Model files not found - API cannot start",
            extra={'error': str(e)}
        )
        raise RuntimeError("Cannot start API without model files")
        
    except Exception as e:
        MODEL_LOADED.set(0)
        logger.critical(
            "Failed to load model",
            extra={'error': str(e)},
            exc_info=True
        )
        raise RuntimeError(f"Model loading error: {e}")
    
    # ========================================
    # CONNECT TO REDIS (OPTIONAL)
    # ========================================
    
    logger.info(f"Connecting to Redis at {settings.REDIS_HOST}:{settings.REDIS_PORT}")
    
    try:
        REDIS_CLIENT = redis.Redis(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            db=settings.REDIS_DB,
            decode_responses=True,
            socket_timeout=5,
            socket_connect_timeout=5
        )
        
        # Test connection
        REDIS_CLIENT.ping()
        
        # Update metric
        REDIS_CONNECTED.set(1)
        
        logger.info("Redis connected successfully")
        
    except redis.ConnectionError as e:
        REDIS_CONNECTED.set(0)
        logger.warning(
            "Failed to connect to Redis - API will run without caching",
            extra={'error': str(e)}
        )
        REDIS_CLIENT = None
        
    except Exception as e:
        REDIS_CONNECTED.set(0)
        logger.warning(
            "Unexpected Redis error - API will run without caching",
            extra={'error': str(e)}
        )
        REDIS_CLIENT = None
    
    logger.info("API ready to serve requests")


@app.on_event("shutdown")
async def shutdown_event():
    """
    Cleanup on application shutdown.
    
    Changes from 4.1:
    + Update metrics on shutdown
    """
    global REDIS_CLIENT
    
    logger.info("=" * 60)
    logger.info("SHUTTING DOWN PLAYER PROP API")
    logger.info("=" * 60)
    
    # Update metrics
    MODEL_LOADED.set(0)
    REDIS_CONNECTED.set(0)
    
    if REDIS_CLIENT is not None:
        try:
            REDIS_CLIENT.close()
            logger.info("Redis connection closed")
        except Exception as e:
            logger.warning(
                "Error closing Redis",
                extra={'error': str(e)}
            )
    
    logger.info("Shutdown complete")


# ============================================================================
# ENDPOINTS
# ============================================================================

@app.get("/health")
async def health_check():
    """
    Health check endpoint for monitoring.
    
    Changes from 4.1:
    + Returns actual metric values
    """
    # Check Redis connection
    redis_connected = False
    if REDIS_CLIENT is not None:
        try:
            REDIS_CLIENT.ping()
            redis_connected = True
        except:
            redis_connected = False
    
    health_status = {
        "status": "healthy" if MODEL is not None else "unhealthy",
        "timestamp": datetime.now().isoformat(),
        "model_loaded": MODEL is not None,
        "redis_connected": redis_connected,
        "version": "1.0.0"
    }
    
    # Return 503 if model not loaded
    if MODEL is None:
        health_status["errors"] = ["Model not loaded"]
        return JSONResponse(
            status_code=503,
            content=health_status
        )
    
    # Warning if Redis down (not critical)
    if not redis_connected:
        health_status["warnings"] = ["Redis not connected - caching disabled"]
    
    return health_status


@app.get("/metrics")
async def metrics():
    """
    Prometheus metrics endpoint.
    
    NEW in Phase 4.3.
    
    Returns:
        Prometheus exposition format (text/plain)
    
    Example output:
        # HELP prop_api_requests_total Total number of HTTP requests
        # TYPE prop_api_requests_total counter
        prop_api_requests_total{method="POST",endpoint="/predict/player",status_code="200"} 42.0
        ...
    
    Usage:
        curl http://localhost:8000/metrics
        
        # Or configure Prometheus to scrape this endpoint
        scrape_configs:
          - job_name: 'prop_api'
            static_configs:
              - targets: ['api:8000']
    """
    return Response(
        content=get_metrics(),
        media_type="text/plain; version=0.0.4; charset=utf-8"
    )


@app.post("/predict/player", response_model=PlayerPredictionResponse)
async def predict_player(request: PlayerPredictionRequest, http_request: Request):
    """
    Predict props for a single player with Redis caching.
    
    Changes from 4.1:
    + Track cache hits/misses in metrics
    + Track inference duration per prop
    + Use RequestLogger for structured logging
    + Record prediction errors by type
    """
    # Get request ID (from middleware)
    request_id = getattr(http_request.state, 'request_id', 'unknown')
    req_logger = RequestLogger(logger, request_id)
    
    # Check model loaded
    if MODEL is None:
        req_logger.error("Prediction requested but model not loaded")
        record_prediction_error('model')
        raise HTTPException(
            status_code=503,
            detail="Model not loaded. API is starting up or failed to load model."
        )
    
    # ========================================
    # CHECK CACHE (if Redis available)
    # ========================================
    
    cache_key = f"pred:tier1:{request.player_id}:{request.opponent_id}:{request.was_home}:v1.0"
    cached_result = None
    
    if REDIS_CLIENT is not None:
        try:
            cached_result = REDIS_CLIENT.get(cache_key)
            if cached_result:
                # Cache HIT
                record_cache_hit()
                req_logger.info(
                    "Prediction served from cache",
                    player_id=request.player_id,
                    cache_key=cache_key
                )
                response_dict = json.loads(cached_result)
                response_dict['cached'] = True
                return PlayerPredictionResponse(**response_dict)
        except Exception as e:
            record_cache_error('read')
            req_logger.warning(
                "Cache read failed",
                error=str(e),
                cache_key=cache_key
            )
            # Continue without cache
    
    # ========================================
    # CACHE MISS: Make fresh prediction
    # ========================================
    
    record_cache_miss()
    
    try:
        start_time = time.time()
        
        # Make prediction
        result = MODEL.predict_player(
            player_id=request.player_id,
            opponent_id=request.opponent_id,
            position=request.position,
            was_home=request.was_home,
            features=request.features
        )
        
        inference_time_ms = int((time.time() - start_time) * 1000)
        
        # Track inference time per prop (for debugging slow predictions)
        for prop_type in ['goals', 'shots', 'cards']:
            INFERENCE_DURATION.labels(prop_type=prop_type).observe(
                inference_time_ms / 1000.0 / 3  # Approximate per-prop time
            )
        
        # Convert to response format
        response = PlayerPredictionResponse(
            player_id=request.player_id,
            opponent_id=request.opponent_id,
            position=request.position,
            was_home=request.was_home,
            predictions={
                "goals": PropPrediction(**result['goals']),
                "shots": PropPrediction(**result['shots']),
                "cards": PropPrediction(**result['cards'])
            },
            cached=False,
            inference_time_ms=inference_time_ms,
            model_version="v1.0",
            timestamp=datetime.now().isoformat()
        )
        
        # ========================================
        # STORE IN CACHE (if Redis available)
        # ========================================
        
        if REDIS_CLIENT is not None:
            try:
                response_json = response.json()
                REDIS_CLIENT.setex(cache_key, 300, response_json)
                req_logger.info(
                    "Prediction cached",
                    cache_key=cache_key,
                    ttl_seconds=300
                )
            except Exception as e:
                record_cache_error('write')
                req_logger.warning(
                    "Cache write failed",
                    error=str(e),
                    cache_key=cache_key
                )
        
        req_logger.info(
            "Prediction completed",
            player_id=request.player_id,
            inference_time_ms=inference_time_ms,
            cached=False
        )
        
        return response
        
    except ValueError as e:
        # Input validation error
        record_prediction_error('validation')
        req_logger.warning(
            "Invalid input for prediction",
            error=str(e),
            player_id=request.player_id
        )
        raise HTTPException(
            status_code=400,
            detail=f"Invalid input: {str(e)}"
        )
        
    except Exception as e:
        # Unexpected error
        record_prediction_error('unknown')
        req_logger.error(
            "Prediction failed with unexpected error",
            error=str(e),
            player_id=request.player_id,
            exc_info=True
        )
        raise HTTPException(
            status_code=500,
            detail=f"Prediction failed: {str(e)}"
        )


# ============================================================================
# RUN (for development only)
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host=settings.API_HOST,
        port=settings.API_PORT,
        log_level=settings.LOG_LEVEL.lower()
    )